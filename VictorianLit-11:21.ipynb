{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Victorian Literature Project</h1>\n",
    "<h3>Zirak, Aleksandr, Jialin</h3>\n",
    "\n",
    "<p>This notebook cotains all the code for the Data Science II final project</p>\n",
    "\n",
    "<p>Our topic was Victorian Literature where we are to extract different themes from the works of different Victorian-era writers. The data is pulled from the GitHub repository of Project Gutenberg at https://github.com/DigiUGA/Gutenberg_Text</p>\n",
    "\n",
    "<p>The biggest challenge that was presented was quantifying what it means for a word to be part of a theme. Since our data was so large, we had to stay away from supervised learning methods since labelling large amounts of data was unrealistic. We then settled on solving the problem with a semi-supervised approach where we would manually label portions of the data to help the model learn themes, and then have it generalize to other words in the text corpus.</p>\n",
    "\n",
    "<p>Our approach starts with pulling the dataset from GitHub. </p>\n",
    "\n",
    "</p>We then passed the text through several filters created with NLTK. These filters include removing puncutation, tokenizing all the words, conversion to lowercase, removing stop words pulled from two libraries (aproximately 1050 words), and finally removing all numbers. We tried to apply stemming which would find the root of a word, but felt that it did not sufficiently consolidate the total vocabulary size. The words were finally grouped in a set which constricted it to only unique words, of which there were approximately </p>\n",
    "\n",
    "<p>Next, we loaded the pretrained GloVe embedding obtained from Stanford. The embedding was trained on 840 billion tokens and mapped approximately 200 million unique words to 300 dimensions. Then, for every single word in our vocabulary set, we found the resulting embedding and stacked an embedding for each word into a matrix while also noting which word appeared on which row.</p>\n",
    "\n",
    "\n",
    "<p>With an mapping of all relavent vocabulary words that appear in our dataset to their embeddings, the resulting matrix was run through the radial basis function kernel that generated an affinity matrix. Fortunately, because the difference between word A and word B is the same as the difference between word B and word A, the resulting graph after the kernel function is symmetric which simplifies the MRW</p>\n",
    "\n",
    "<p>The MultiRank Walk then takes the affinity matrix and our labelled words and generates a series of u vectors which can be used to cluster each word in our vocab set into our predefined classes. Now, the text of different Victorian authors can be clustered into different categories.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "import sys\n",
    "import wget\n",
    "from urllib import request\n",
    "import zipfile\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Text Corpus preprocessing</h1>\n",
    "\n",
    "<p>Downloads necessary texts and NLTK packages. Reads all the files and generates tokens which are then passed through a series of filters that remove punctuation, numbers, stop words, converts to lower case, and removes duplicates.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloning repository containing all the text files\n",
    "!git clone https://github.com/YJL0629/English-Writers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading necessary collections of stop words that will be removed from text\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Consolidating text files which will make up our text corpus. \n",
    "\"\"\"\n",
    "\n",
    "raw = \"\"\n",
    "\n",
    "for file in glob(\"./English-Writers/*/*\"):\n",
    "    with open(file, 'r') as f:\n",
    "        raw += f.read().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parsing raw text and preprocessing into a workable set.\n",
    "\"\"\"\n",
    "\n",
    "def process_text(text):\n",
    "    \n",
    "    # Tokenizing words.\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    # Sending words to lowercase and removing duplicates.\n",
    "    vocab = [w.lower() for w in tokens]\n",
    "\n",
    "    # Generating list of \"stop\" words.\n",
    "    stop_words = list(get_stop_words('en'))         #About 900 stopwords\n",
    "    nltk_words = list(stopwords.words('english')) #About 150 stopwords\n",
    "    stop_words.extend(nltk_words)\n",
    "\n",
    "    # Removing unecessary \"stop\" words.\n",
    "    vocab_wo_stopwords = [w for w in vocab if not w in stop_words]\n",
    "\n",
    "    # Removing numbers.\n",
    "    vocab = [word for word in vocab_wo_stopwords if word.isalpha()]\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicates.\n",
    "vocab_set = process_text(raw)\n",
    "\n",
    "# Logging.\n",
    "print(\"=\" * 20)\n",
    "print(\"The set of the vocab words has %d unique words\" % len(vocab_set))\n",
    "print(\"=\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Obtaining Embedding Data</h1>\n",
    "\n",
    "<p>This portion of the notebook pulls the embeddings from Stanford and unzips them into a folder called 'embedding'.</p>\n",
    "\n",
    "<p>Note- this takes a long time.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Url that contains the embeddings.\n",
    "url = 'http://nlp.stanford.edu/data/glove.840B.300d.zip'\n",
    "\n",
    "# Download.\n",
    "wget.download(url, './glove.840B.300d.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzipping.\n",
    "with zipfile.ZipFile('./glove.840B.300d.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('./embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Generating Matrix of Embeddings</h1>\n",
    "\n",
    "<p>Generates a matrix where each row is a word and the columns are the word's embedding in 300 dimensions. Words are only added to this matrix if they appear in the vocabulary set generated by parsing Victorian text.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to embedding .txt file.\n",
    "glove_dir = './embedding'\n",
    "\n",
    "# Maps the word to its embedding (vector).\n",
    "word_to_embedding = {}\n",
    "\n",
    "# Final matrix that is (len(vocab_set) x len(embedding)).\n",
    "full_embedding = None\n",
    "\n",
    "embedding_matrix = None\n",
    "word_to_matrix_row = {}\n",
    "\n",
    "# Looping through all rows of the embeddings.\n",
    "with open(os.path.join(glove_dir, 'glove.840B.300d.txt')) as f:\n",
    "    \n",
    "    index_in_matrix = 0\n",
    "    embedding_line = 0\n",
    "    \n",
    "    # For each word and its embedding.\n",
    "    for line in f:\n",
    "\n",
    "        # Logging.\n",
    "        embedding_line += 1\n",
    "        sys.stdout.write(\"\\r%d\" % embedding_line)\n",
    "        \n",
    "        # Splitting the embedding line.\n",
    "        values = line.split()\n",
    "        \n",
    "        # The first value is the word, the rest are the values of the embedding.\n",
    "        word = values[0] \n",
    "\n",
    "        # If a word that has an embedding does not appear in the vocab set .\n",
    "        if word not in vocab_set:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Read embedding for that word.\n",
    "            embedding = np.asarray(values[1:], dtype='float32')\n",
    "\n",
    "        # Some embeddings have unusual formatting.\n",
    "        except Exception:\n",
    "            print(\"\\nCorrupted embedding for %s\" % word)\n",
    "            continue\n",
    "            \n",
    "        # If no rows have been added to the matrix of the embeddings.\n",
    "        if embedding_matrix is None:\n",
    "            \n",
    "            # Initialize the embedding matrix.\n",
    "            embedding_matrix = np.array([embedding])\n",
    "            \n",
    "            # Map the word to an index in the matrix.\n",
    "            word_to_matrix_row[word] = index_in_matrix\n",
    "            index_in_matrix += 1\n",
    "            \n",
    "        # If this word contains a proper embedding.\n",
    "        elif len(embedding) == 300:\n",
    "            \n",
    "            # Add the embedding to our matrix.\n",
    "            embedding_matrix = np.vstack((embedding_matrix, embedding))\n",
    "            \n",
    "            # Map the word to an index in the matrix.\n",
    "            word_to_matrix_row[word] = index_in_matrix\n",
    "            index_in_matrix += 1\n",
    "            \n",
    "        else:\n",
    "            print(\"\\nCould not add %s to the embedding matrix\" % word)\n",
    "            \n",
    "# Set of our vocab words that are represented in the embedding matrix,\n",
    "# will be smaller that vocab_set because some words do not have an embedding.\n",
    "represented_vocab_set = set(word_to_matrix_row.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Checkpoint</h1>\n",
    "\n",
    "<p>Saving the mapping of words to which line line of the embedding matrix they appear in.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Checkpoints/word_to_matrix_row', word_to_matrix_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Logging Results Conversion of Vocab to Embedding Matrix</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info about original vocab set and the new one after removing words\n",
    "# that did not have an embedding.\n",
    "print(\"=\" * 20)\n",
    "print(\"The original vocab had %d words\" % len(vocab_set))\n",
    "print(\"The updated vocab set has %d words\" % len(represented_vocab_set))\n",
    "print(\"=\" * 20)\n",
    "\n",
    "# Words that are in the vocab set but did not make it into the embedding matrix\n",
    "# may be due to the embedding being corrupted or an embedding for that word may not exist.\n",
    "print(\"=\" * 20)\n",
    "print(\"%d words have been thrown out due to not having an embedding\" % (len(vocab_set) - len(represented_vocab_set)))\n",
    "print(\"=\" * 20)\n",
    "\n",
    "# Dimension of the embedding matrix.\n",
    "embedding_matrix_shape = np.shape(embedding_matrix)\n",
    "\n",
    "print(\"=\" * 20)\n",
    "print(\"The embedding matrix is %d by %d\" % (embedding_matrix_shape[0], embedding_matrix_shape[1]))\n",
    "print(\"=\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Generating Affinity Matrix</h1>\n",
    "\n",
    "<p>Uses the embedding matrix to generate an affinity matrix by using the radius basis function kernel. This displays a graph of the relationships between words.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the Radial Basis Function kernel to generate an affinity matrix.\n",
    "affinity_matrix = rbf_kernel(X=embedding_matrix, gamma=0.01)\n",
    "\n",
    "plt.imshow(affinity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Checkpoint</h1>\n",
    "\n",
    "<p>Saving affinity matrix so that it will not be computed later.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Checkpoints/affinity_matrix', affinity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Generating Dataset</h1>\n",
    "\n",
    "<p>Loading hand-labelled samples from text files on the disk and giving each of the classes a unique id. All other words that appear in the represented_vocab_set (rows and columns of the affinity matrix) are labelled as unknown (-1).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Handles processing labelled samples.\n",
    "\"\"\"\n",
    "\n",
    "# Set of our vocab words that are represented in the embedding matrix.\n",
    "represented_vocab_set = set(word_to_matrix_row.keys())\n",
    "\n",
    "# Maps each loaded labelled sampled to a unique id for each theme.\n",
    "word_to_theme_id = {}\n",
    "\n",
    "# Maps the unique theme id to its actual name.\n",
    "id_to_theme_name = {-1 : 'Unknown'}\n",
    "\n",
    "# Looping through all the labelled text files.\n",
    "for i, file in enumerate(glob(\"./Labelled-Data/*\")):\n",
    "    with open(file, 'r') as f:\n",
    "        \n",
    "        # Extracting the name of the theme from the filename.\n",
    "        theme_name = file.split('/')[-1][ : -4]\n",
    "        \n",
    "        # Maps a unique id to each theme.\n",
    "        id_to_theme_name[i] = theme_name\n",
    "        \n",
    "        # Goes through all words in the file.\n",
    "        for word in f.read().split(\"\\n\"):\n",
    "            \n",
    "            # Reformatting text.\n",
    "            word = word.lower().strip()\n",
    "            \n",
    "            # Removing blank lines\n",
    "            if word == '':\n",
    "                continue\n",
    "                \n",
    "            # Checking that the word that we hand label has an entry\n",
    "            # in the affinity matrix.\n",
    "            if word not in represented_vocab_set:\n",
    "                print('%s is not in the affinity matrix.' % word)\n",
    "                continue\n",
    "            \n",
    "            # Mapping the word to its unique theme id\n",
    "            word_to_theme_id[word] = i\n",
    "            \n",
    "# Set of words that have a label\n",
    "labelled_words_set = set(word_to_theme_id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Handles processing unlabelled samples.\n",
    "\"\"\"\n",
    "\n",
    "# Loops through all words that are represented in the affinity matrix.\n",
    "for word in represented_vocab_set:\n",
    "    \n",
    "    # If a word is not labelled.\n",
    "    if word not in labelled_words_set:\n",
    "        \n",
    "        # Assign it an unknown label (-1).\n",
    "        word_to_theme_id[word] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>MultiRank Walk</h1>\n",
    "\n",
    "<p>***magic***</p>\n",
    "\n",
    "<p>How does this work?</p>\n",
    "<p>¯\\_(ツ)_/¯</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initializing variables needed for the MRW.\n",
    "\"\"\"\n",
    "\n",
    "num_data_points = len(word_to_theme_id)\n",
    "represented_vocab_set = set(word_to_matrix_row.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Using the affinity matrix to calculate the diagonal matrix D \n",
    "# by summing the rows of the affinity matrix and\n",
    "# putting then on the diagonal.\n",
    "D = np.diag([np.sum(x) for x in affinity_matrix])\n",
    "\n",
    "# Using the affinity matrix and D to calculate the weighted transition matrix W.\n",
    "W = np.zeros((num_data_points, num_data_points))\n",
    "\n",
    "# Looping through all elements in the affinity matrix\n",
    "for row in range(num_data_points):\n",
    "    \n",
    "    # Logging.\n",
    "    sys.stdout.write(\"\\r%d/%d\" % (row + 1, num_data_points))\n",
    "    \n",
    "    for col in range(num_data_points):\n",
    "        W[row][col] = affinity_matrix[row][col] / D[row][row]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Checkpoint</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./Checkpoints/W', W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Continuing MRW</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.load('./Checkpoints/W.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populating list of u vectors.\n",
    "u_vectors = []\n",
    "\n",
    "# Seeding all u vectors\n",
    "# The length of the dictionary contains the number of known\n",
    "# classes plus one for the unkown. This loop will go through\n",
    "# all known classes.\n",
    "# There will be one u vector for each class.\n",
    "for class_id in range(len(id_to_theme_name.keys()) - 1):\n",
    "\n",
    "    # Initializing a u vector to the same size of indices as there are\n",
    "    # represented words in the affinity matrix\n",
    "    u = np.zeros((num_data_points, 1))\n",
    "    \n",
    "    # For each u vector, change the index to 1 at the points\n",
    "    # where a word of this class_id appears in the affinity matrix.\n",
    "    # \n",
    "    # This loops through all the words represented in the affinity matrix.\n",
    "    for word in represented_vocab_set:\n",
    "        \n",
    "        # If a word belong to this class_id.\n",
    "        if word_to_theme_id[word] == class_id:\n",
    "            \n",
    "            # Update the u vector at the same index at which the word\n",
    "            # appears in the affinity matrix.\n",
    "            u[word_to_matrix_row[word]][0] = 1\n",
    "    \n",
    "    # Generating matrix of u vectors.\n",
    "    u_vectors.append(u)\n",
    "\n",
    "# Normalizing each u vector.\n",
    "for i in range(len(u_vectors)):\n",
    "    norm_factor = np.linalg.norm(u_vectors[i], ord=1)\n",
    "    u_vectors[i] /= norm_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Finding the ranking vectors r.\n",
    "\"\"\"\n",
    "\n",
    "r_vectors = []\n",
    "\n",
    "# Initializing r vectors.\n",
    "for _ in u_vectors:\n",
    "    \n",
    "    r_vectors.append(np.ones((num_data_points, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "num_iterations = 100\n",
    "damping = 0.95\n",
    "\n",
    "# Looping until convergence.\n",
    "for iteration in range(num_iterations):\n",
    "    \n",
    "    sys.stdout.write('\\r%d/%d' % (iteration + 1, num_iterations))\n",
    "    \n",
    "    # Setting new values for each r/u vector.\n",
    "    for i in range(len(r_vectors)):\n",
    "        r_vectors[i] = (1 - damping) * u_vectors[i] + np.transpose(damping * np.matmul(np.transpose(r_vectors[i]), W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Checkpoint</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./Checkpoints/r_vectors', r_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Evaluation</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_vectors = np.load('./Checkpoints/r_vectors.npy')\n",
    "word_to_matrix_row = np.load('./Checkpoints/word_to_matrix_row.npy', allow_pickle=True).item()\n",
    "represented_vocab_set = set(word_to_matrix_row.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_category(word):\n",
    "    \n",
    "    options = []\n",
    "    word_idx = word_to_matrix_row[word]\n",
    "\n",
    "    for r in r_vectors:\n",
    "        options.append(r[word_idx])\n",
    "\n",
    "    largest = max(options)\n",
    "    smallest = min(options)\n",
    "    \n",
    "    \n",
    "    ret = None\n",
    "    \n",
    "    for i, option in enumerate(options):\n",
    "        if option == largest:            \n",
    "            ret = id_to_theme_name[i]\n",
    "            break\n",
    "        \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prediction_for_text(text):\n",
    "    \n",
    "    vocab = process_text(text)\n",
    "    \n",
    "    categories = []\n",
    "    \n",
    "    for word in vocab:\n",
    "        \n",
    "        if word in represented_vocab_set:\n",
    "            category = predict_category(word)\n",
    "            categories.append(category)\n",
    "            \n",
    "    print(\"=\" * 20)\n",
    "            \n",
    "    # print(categories)\n",
    "    for category in set(categories):\n",
    "        print(\"%s : %.2f%%\" % (category, (categories.count(category) / len(categories)) * 100))\n",
    "        \n",
    "    print(\"=\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prediction_for_file(file_path):\n",
    "    \n",
    "    print(\"=\" * 20)\n",
    "    print(\"%s\" % file_path.split(\"/\")[-1][:-4])\n",
    "    print(\"=\" * 20)\n",
    "    \n",
    "    raw = \"\"\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        raw += f.read().replace('\\n', ' ')\n",
    "    \n",
    "    generate_prediction_for_text(raw)\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = './English-Writers/*'\n",
    "\n",
    "for author_path in glob(\"./English-Writers/*\"):\n",
    "    \n",
    "    author_name = author_path.split('/')[-1]\n",
    "    \n",
    "    print(\"*\" * 30)\n",
    "    print(author_name)\n",
    "    print(\"*\" * 30)\n",
    "    \n",
    "    for text_file_path in glob('./English-Writers/%s/*.txt' % author_name):\n",
    "        generate_prediction_for_file(text_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
